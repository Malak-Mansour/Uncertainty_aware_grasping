## Conda environment setup
```
source anaconda3/bin/activate

conda create -n uncertainty_grasping python=3.10 -y #we are calling it sam2 now
conda activate uncertainty_grasping #we are calling it sam2 now
```


make python work with ros
```
python3 -m pip install rosdep rosinstall-generator vcstool
source /opt/ros/humble/setup.bash
sudo apt update
sudo apt install ros-humble-desktop python3-rosdep python3-colcon-common-extensions
pip install colcon-common-extensions empy lark pyyaml
```


conda install -n base conda-libmamba-solver 
conda config --set solver libmamba 
conda install nvidia/label/cuda-12.4.0::cuda-toolkit 

export CUDA_HOME=$CONDA_PREFIX 
export PATH=$CONDA_PREFIX/bin:$PATH 
export C_INCLUDE_PATH=$CONDA_PREFIX/include:$C_INCLUDE_PATH 
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH

conda install pytorch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 pytorch-cuda=12.4 -c pytorch -c nvidia



pip install -e sam2
pip install -e "sam2[notebooks]"
pip install -e contact_graspnet_pytorch

pip install -r requirements.txt



## Dataset and segmentation (fix the repos uploaded later and the weights)
pip install ultralytics
robotic_picker/src/strawberry_detector/main.py: listens to the camera topics and segments them (but add the 2 weights in the same folder)
   sam2_hiera_small.pt
   sam2_hiera_s.yaml
   yolo_model.pt

outside Uncertainty_aware_grasping
mkdir armyolo_ws

colcon build
source install/setup.bash
ros2 run yolov8_ros yolov8_nodes


## Simulate leaf occlusions to partial pointcloud
Set the occlusion amount (0.1, 0.2, 0.3, 0.4) as the leaf_coverage argument in the _create_leaf_occlusion_ function call.
However, no need to run this step since the folders are already in PointAttN data folders.
```
python simulate_occlusions.py
```

## PointAttN
We are running it with no occlusions, occlusion 0.1, 0.2, 0.3, and 0.4.
```
cd PointAttN-Modified_uncertainty
```
1. uncomment the 2 data_dir lines in train_not_noisy.py with the experiment (what level of occlusions) that u want
2. uncomment the data_dir line in _test_pcn_MC.py_ with the experiment (what level of occlusions) that u want
3. (un)comment the "If doing occlusions" section in _PCDDataset.py_ depending on your experiment
4. Write the model name in _cfgs/PointAttN.yaml_:
     PointAttN_baseline_cd_matching_f1 (without dropout)
   or
     PointAttN_baseline_cd_matching_f1_MC (with dropout)
5. ```
   python train_not_noisy.py -c PointAttN.yaml
   ```
6. Choose the path of the latest epoch in the training results and add that to the config file
   ```
   python test_pcn_MC.py -c PointAttN.yaml
   ```


## GraspNet
Modify the paths in the following files. Note that .npz are the completion outputs from PointAttN
```
cd contact_graspnet_pytorch
```
1. ```
   python move_npz_to_npy.py  #includes filtering the pcl
   ```
2. ```
   python contact_graspnet_pytorch/inference.py --np_path=npy_files/*.npy --forward_passes=5 --z_range=[0.2,1.1]​
   ```
3. Add the .npz fields from the completed point cloud to the ones generated by GraspNet all in the same new .npz files in the folder _results_with_completion_
   ```
   python add_completion_fields.py
   ```

​
## Visualize and filter grasps ​
```
python filter_grasps.py
```
After filtering out the bad grasps, choose the strawberry (.npz) with the least stddev to pick first
```
python rank_stddev.py
```
