## Dataset and segmentation
The directories data and data_test in [**PointAttN-Modified/data_sim**](https://mbzuaiac-my.sharepoint.com/personal/ali_abouzeid_mbzuai_ac_ae/_layouts/15/onedrive.aspx?e=5%3A38880696f9dd4ad9b92df73c8145c0f5&sharingv2=true&fromShare=true&at=9&CT=1739939191678&OR=OWA%2DNT%2DMail&CID=159ec6e8%2Df128%2D6d08%2D7b2e%2Da13bc2d9b57a&clickParams=eyJYLUFwcE5hbWUiOiJNaWNyb3NvZnQgT3V0bG9vayBXZWIgQXBwIiwiWC1BcHBWZXJzaW9uIjoiMjAyNTAyMDYwNzYuMDkiLCJPUyI6IldpbmRvd3MgMTEifQ%3D%3D&cidOR=Client&FolderCTID=0x01200061DDB91AB87ED34E9DF35DE2B429C65D&id=%2Fpersonal%2Fali%5Fabouzeid%5Fmbzuai%5Fac%5Fae%2FDocuments%2Fbags%5Fmalak%2Fdata%5Fsim%2Ezip&parent=%2Fpersonal%2Fali%5Fabouzeid%5Fmbzuai%5Fac%5Fae%2FDocuments%2Fbags%5Fmalak) and [**PointAttN-Modified/data_mix**](https://mbzuaiac-my.sharepoint.com/personal/ali_abouzeid_mbzuai_ac_ae/_layouts/15/onedrive.aspx?e=5%3A38880696f9dd4ad9b92df73c8145c0f5&sharingv2=true&fromShare=true&at=9&CT=1739939191678&OR=OWA%2DNT%2DMail&CID=159ec6e8%2Df128%2D6d08%2D7b2e%2Da13bc2d9b57a&clickParams=eyJYLUFwcE5hbWUiOiJNaWNyb3NvZnQgT3V0bG9vayBXZWIgQXBwIiwiWC1BcHBWZXJzaW9uIjoiMjAyNTAyMDYwNzYuMDkiLCJPUyI6IldpbmRvd3MgMTEifQ%3D%3D&cidOR=Client&FolderCTID=0x01200061DDB91AB87ED34E9DF35DE2B429C65D&id=%2Fpersonal%2Fali%5Fabouzeid%5Fmbzuai%5Fac%5Fae%2FDocuments%2Fbags%5Fmalak%2Fdata%5Fmix%2Ezip&parent=%2Fpersonal%2Fali%5Fabouzeid%5Fmbzuai%5Fac%5Fae%2FDocuments%2Fbags%5Fmalak) contain the pure simulation data and mixed (sim+real) datasets, respectively, that were used in experimentations

1. Collecting sim data from IsaacSim strawberry field setup. Dataset link: https://app.roboflow.com/strawberry-detection-aghjb/strawberry-detection2-iilvu/browse?queryText=&pageSize=50&startingIndex=0&browseQuery=true

2. Collecting real data method and a description of what each step does: 
  - **IntelRealsenseViewer:** collect a bag using the Intel Realsense camera and the IntelRealsenseViewer software on Windows
  - **sam2/sam2/filterpoints.py:** extract this bag into RGB and Depth images, which can extract a point cloud. Run SAM2 on the images to segment strawberries, which you can use to segment the strawberries in the pointcloud
    - For verification purposes, you can also extract the bag into RGB and Depth images, which can extract a point cloud using **extract_IntelRealsense_bag/view_bag.ipynb**, then view it using **extract_IntelRealsense_bag/visualize_pointcloud_files.ipynb**
  - **sam2/sam2/transform_model_to_real.py:** to manually label the segmented point clouds (match them on top of the ground truth strawberry model) using the x, y, z position and angle scroll bars, then save the transformed model and matrix
  - **PointAttN-Modified/test.ipynb:** to convert the saved transformations into a dataset that will be accepted as input for PointAttn 


## Denoise partial pointclouds
```
python filter_points.py
```

## Simulate leaf occlusions to partial pointcloud
Set the occlusion amount (0.1, 0.2, 0.3, 0.4) as the leaf_coverage argument in the _create_leaf_occlusion_ function call.
However, no need to run this step since the folders are already in PointAttN data folders.
```
python simulate_occlusions.py
```

## PointAttN
We are running it with no occlusions, occlusion 0.1, 0.2, 0.3, and 0.4.
```
cd PointAttN-Modified_uncertainty
```
1. uncomment the 2 data_dir lines in train_not_noisy.py with the experiment (what level of occlusions) that u want
2. uncomment the data_dir line in _test_pcn_MC.py_ with the experiment (what level of occlusions) that u want
3. (un)comment the "If doing occlusions" section in _PCDDataset.py_ depending on your experiment
4. Write the model name in _cfgs/PointAttN.yaml_:
     PointAttN_baseline_cd_matching_f1 (without dropout)
   or
     PointAttN_baseline_cd_matching_f1_MC (with dropout)
5. ```
   python train_not_noisy.py -c PointAttN.yaml
   ```
6. Choose the path of the latest epoch in the training results and add that to the config file
   ```
   python test_pcn_MC.py -c PointAttN.yaml
   ```


## GraspNet
Modify the paths in the following files. Note that .npz are the completion outputs from PointAttN
```
cd contact_graspnet_pytorch
```
1. ```
   python move_npz_to_npy.py  #includes filtering the pcl
   ```
2. ```
   python contact_graspnet_pytorch/inference.py --np_path=npy_files/*.npy --forward_passes=5 --z_range=[0.2,1.1]​
   ```
3. Add the .npz fields from the completed point cloud to the ones generated by GraspNet all in the same new .npz files in the folder _results_with_completion_
   ```
   python add_completion_fields.py
   ```

​
## Visualize and filter grasps ​
```
python filter_grasps.py
```
After filtering out the bad grasps, choose the strawberry (.npz) with the least stddev to pick first
```
python rank_stddev.py
```
